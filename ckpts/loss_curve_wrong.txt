import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
import os
import glob
import re
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence
import matplotlib.pyplot as plt

class AudioSequenceDataset(Dataset):
    def __init__(self, data_dir, max_seq_length=6000):
        self.data_dir = data_dir
        self.max_seq_length = max_seq_length
        self.file_paths = []
        self.targets = []
        
        # Get all CSV files
        csv_files = glob.glob(os.path.join(data_dir, "*.csv"))
        
        for file_path in csv_files:
            # Extract f and Z from filename
            filename = os.path.basename(file_path)
            match = re.search(r"output_f=(\d+)_Z=(\d+)_T=(\d+\.\d+)\.csv", filename)
            if match:
                f = float(match.group(1))
                Z = float(match.group(2))
                
                self.file_paths.append(file_path)
                self.targets.append((f, Z))
    
    def __len__(self):
        return len(self.file_paths)
    
    def __getitem__(self, idx):
        file_path = self.file_paths[idx]
        target = self.targets[idx]
        
        # Read CSV file, skip header rows
        df = pd.read_csv(file_path, skiprows=4)
        
        # Get probe data as features
        # time = df.iloc[:, 0].values  # First column is time
        # probe1 = df.iloc[:, 1].values  # Second column is probe 1
        probe2 = df.iloc[:, 2].values  # Third column is probe 2
        
        # Combine features - reshape to 2D tensor with shape (seq_len, 1)
        features = probe2.reshape(-1, 1)

        # Convert to tensor
        features = torch.tensor(features, dtype=torch.float32)
        target = torch.tensor(target, dtype=torch.float32)
        # print(features.shape)
        # print(target)
        # # plt visualize the features
        # plt.plot(time, features)
        # plt.show()
        
        # Return features, target, and sequence length
        return features, target, len(features)

def collate_fn(batch):
    # Sort batch by sequence length (descending)
    batch.sort(key=lambda x: x[2], reverse=True)
    
    # Separate features, targets, and lengths
    features, targets, lengths = zip(*batch)
    
    # Pad sequences
    features_padded = pad_sequence(features, batch_first=True)
    
    # Convert to tensors
    targets = torch.stack(targets)
    lengths = torch.tensor(lengths)
    
    return features_padded, targets, lengths

class AudioSeqModel(nn.Module):
    def __init__(self, input_size=1, hidden_size=128, num_layers=2, dropout=0.3):
        super(AudioSeqModel, self).__init__()
        
        # 1D CNN for feature extraction
        self.conv_layers = nn.Sequential(
            nn.Conv1d(input_size, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.BatchNorm1d(64),
            nn.Conv1d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.BatchNorm1d(128),
            nn.Conv1d(128, hidden_size, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.BatchNorm1d(hidden_size)
        )
        
        # Bidirectional LSTM for sequence modeling
        self.lstm = nn.LSTM(
            input_size=hidden_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=True
        )
        
        # Attention mechanism
        self.attention = nn.Sequential(
            nn.Linear(hidden_size * 2, 1),
            nn.Tanh()
        )
        
        # Fully connected layers for prediction
        self.fc_layers = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size, 64),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(64, 2)  # Output: f and Z
        )
    
    def forward(self, x, lengths):
        # x shape: [batch_size, seq_len, 1]
        batch_size, seq_len, features = x.size()
        
        # Reshape for CNN: [batch, features, seq_len]
        x = x.permute(0, 2, 1)
        
        # Apply CNN
        x = self.conv_layers(x)
        
        # Reshape back for LSTM: [batch, seq_len, hidden_size]
        x = x.permute(0, 2, 1)
        
        # Make sure lengths is on CPU
        lengths_cpu = lengths.cpu()
        
        # Pack padded sequence
        packed_x = pack_padded_sequence(x, lengths_cpu, batch_first=True)
        
        # Apply LSTM
        lstm_out, (hidden, _) = self.lstm(packed_x)
        
        # Unpack sequence
        lstm_out, _ = pad_packed_sequence(lstm_out, batch_first=True)
        
        # Attention mechanism
        attention_weights = self.attention(lstm_out)
        attention_weights = torch.softmax(attention_weights, dim=1)
        
        # Apply attention weights
        context_vector = torch.sum(attention_weights * lstm_out, dim=1)
        
        # Final prediction
        output = self.fc_layers(context_vector)
        
        return output

def train_model(model, train_loader, val_loader, num_epochs=50, learning_rate=0.001):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    
    # Loss function and optimizer
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    
    # Learning rate scheduler
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)
    
    # Training history
    train_losses = []
    val_losses = []
    
    for epoch in range(num_epochs):
        # Training
        model.train()
        running_loss = 0.0
        
        for features, targets, lengths in train_loader:
            features = features.to(device)
            targets = targets.to(device)
            
            # Forward pass
            outputs = model(features, lengths)
            loss = criterion(outputs, targets)
            
            # Backward pass and optimize
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()
        
        train_loss = running_loss / len(train_loader)
        train_losses.append(train_loss)
        
        # Validation
        model.eval()
        val_loss = 0.0
        
        with torch.no_grad():
            for features, targets, lengths in val_loader:
                features = features.to(device)
                targets = targets.to(device)
                
                outputs = model(features, lengths)
                loss = criterion(outputs, targets)
                
                val_loss += loss.item()
        
        val_loss = val_loss / len(val_loader)
        val_losses.append(val_loss)
        
        # Update learning rate
        scheduler.step(val_loss)
        
        print(f"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")
        
    
    # Plot training history
    plt.figure(figsize=(10, 5))
    plt.plot(train_losses, label='Training Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('MSE Loss')
    plt.title('Training and Validation Loss')
    plt.legend()
    plt.grid(True)
    plt.savefig('training_history.png')
    plt.show()
    
    return model

def evaluate_model(model, test_loader):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    model.eval()
    
    predictions = []
    true_values = []
    
    with torch.no_grad():
        for features, targets, lengths in test_loader:
            features = features.to(device)
            
            outputs = model(features, lengths)
            
            predictions.extend(outputs.cpu().numpy())
            true_values.extend(targets.numpy())
    
    predictions = np.array(predictions)
    true_values = np.array(true_values)
    
    # Calculate RMSE for f and Z
    rmse_f = np.sqrt(np.mean((predictions[:, 0] - true_values[:, 0]) ** 2))
    rmse_z = np.sqrt(np.mean((predictions[:, 1] - true_values[:, 1]) ** 2))
    
    print(f"RMSE for frequency (f): {rmse_f:.2f}")
    print(f"RMSE for impedance (Z): {rmse_z:.2f}")
    
    # Plot predictions vs true values
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.scatter(true_values[:, 0], predictions[:, 0])
    plt.plot([min(true_values[:, 0]), max(true_values[:, 0])], 
             [min(true_values[:, 0]), max(true_values[:, 0])], 'r--')
    plt.xlabel('True f')
    plt.ylabel('Predicted f')
    plt.title('Frequency Prediction')
    plt.grid(True)
    
    plt.subplot(1, 2, 2)
    plt.scatter(true_values[:, 1], predictions[:, 1])
    plt.plot([min(true_values[:, 1]), max(true_values[:, 1])], 
             [min(true_values[:, 1]), max(true_values[:, 1])], 'r--')
    plt.xlabel('True Z')
    plt.ylabel('Predicted Z')
    plt.title('Impedance Prediction')
    plt.grid(True)
    
    plt.tight_layout()
    plt.savefig('prediction_performance.png')
    plt.show()

if __name__ == "__main__":
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # Data directory
    data_dir = "outputs"
    
    # Create dataset
    dataset = AudioSequenceDataset(data_dir)
    
    # Split dataset
    dataset_size = len(dataset)
    train_size = int(dataset_size * 0.7)
    val_size = int(dataset_size * 0.15)
    test_size = dataset_size - train_size - val_size
    
    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(
        dataset, [train_size, val_size, test_size]
    )
    
    # Create data loaders
    train_loader = DataLoader(
        train_dataset, 
        batch_size=16, 
        shuffle=True, 
        collate_fn=collate_fn,
        num_workers=2
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=16, 
        shuffle=False, 
        collate_fn=collate_fn,
        num_workers=2
    )
    
    test_loader = DataLoader(
        test_dataset, 
        batch_size=16, 
        shuffle=False, 
        collate_fn=collate_fn,
        num_workers=2
    )
    
    # Create model
    model = AudioSeqModel(input_size=1, hidden_size=128, num_layers=2, dropout=0.3)
    
    # Train model
    model = train_model(model, train_loader, val_loader, num_epochs=50, learning_rate=0.001)
    
    # Evaluate model
    evaluate_model(model, test_loader)
    
    # Save model
    torch.save(model.state_dict(), 'audio_seq_model.pth')
    print("Model saved to audio_seq_model.pth")






Epoch 1/50, Train Loss: 10359000.0000, Val Loss: 10520679.1429
Epoch 2/50, Train Loss: 9876402.0517, Val Loss: 9500600.1429
Epoch 3/50, Train Loss: 7986693.1207, Val Loss: 6505243.5714
Epoch 4/50, Train Loss: 4277388.7241, Val Loss: 2297166.4286
Epoch 5/50, Train Loss: 1109778.3817, Val Loss: 417220.8929
Epoch 6/50, Train Loss: 614764.8217, Val Loss: 370371.9531
Epoch 7/50, Train Loss: 595641.5086, Val Loss: 379878.9252
Epoch 8/50, Train Loss: 600578.0119, Val Loss: 376068.5190
Epoch 9/50, Train Loss: 622451.3475, Val Loss: 426010.4732
Epoch 10/50, Train Loss: 604681.8470, Val Loss: 376764.4754
Epoch 11/50, Train Loss: 625074.9343, Val Loss: 402322.6373
Epoch 12/50, Train Loss: 618807.8103, Val Loss: 391555.8259
Epoch 13/50, Train Loss: 599932.6638, Val Loss: 394615.4743
Epoch 14/50, Train Loss: 640563.7446, Val Loss: 390219.5391
Epoch 15/50, Train Loss: 630673.3319, Val Loss: 372921.5787
Epoch 16/50, Train Loss: 655618.1627, Val Loss: 369351.4353
Epoch 17/50, Train Loss: 657807.2834, Val Loss: 385831.3884
Epoch 18/50, Train Loss: 596372.4881, Val Loss: 369650.2450
Epoch 19/50, Train Loss: 609903.0938, Val Loss: 369772.7812
Epoch 20/50, Train Loss: 629152.5754, Val Loss: 377720.7221
Epoch 21/50, Train Loss: 585352.8718, Val Loss: 373893.3477
Epoch 22/50, Train Loss: 653134.9332, Val Loss: 374204.7941
Epoch 23/50, Train Loss: 604508.1940, Val Loss: 382640.6719
Epoch 24/50, Train Loss: 609450.2812, Val Loss: 372790.0223
Epoch 25/50, Train Loss: 602256.0323, Val Loss: 10641681.5714
Epoch 26/50, Train Loss: 588133.2328, Val Loss: 374900.4754
Epoch 27/50, Train Loss: 511421.3508, Val Loss: 396519.4442
Epoch 28/50, Train Loss: 570568.0841, Val Loss: 434427.9196
Epoch 29/50, Train Loss: 620950.0081, Val Loss: 400093.7991
Epoch 30/50, Train Loss: 576666.4106, Val Loss: 406867.3795
Epoch 31/50, Train Loss: 499864.8869, Val Loss: 363956.3856
Epoch 32/50, Train Loss: 560286.1579, Val Loss: 357139.9092
Epoch 33/50, Train Loss: 650994.0593, Val Loss: 387369.5815
Epoch 34/50, Train Loss: 598209.0744, Val Loss: 360447.6484
Epoch 35/50, Train Loss: 520129.1056, Val Loss: 360155.8265
Epoch 36/50, Train Loss: 570028.5938, Val Loss: 1505249.4821
Epoch 37/50, Train Loss: 648567.7317, Val Loss: 3519401.1429
Epoch 38/50, Train Loss: 609598.1045, Val Loss: 359148.7651
Epoch 39/50, Train Loss: 605128.1789, Val Loss: 667736.9688
Epoch 40/50, Train Loss: 566461.0905, Val Loss: 386425.5926
Epoch 41/50, Train Loss: 543552.1703, Val Loss: 798532.7321
Epoch 42/50, Train Loss: 554979.2069, Val Loss: 358724.8126
Epoch 43/50, Train Loss: 541120.8491, Val Loss: 358190.1674
Epoch 44/50, Train Loss: 523609.2069, Val Loss: 771812.7679
Epoch 45/50, Train Loss: 526409.3847, Val Loss: 2571924.9286
Epoch 46/50, Train Loss: 521457.3890, Val Loss: 3241397.7143
Epoch 47/50, Train Loss: 491733.7408, Val Loss: 358626.6484
Epoch 48/50, Train Loss: 526439.3254, Val Loss: 350316.4361
Epoch 49/50, Train Loss: 512727.3879, Val Loss: 742423.6696
Epoch 50/50, Train Loss: 524767.6164, Val Loss: 374233.8332
RMSE for frequency (f): 57.96
RMSE for impedance (Z): 815.32
Model saved to audio_seq_model.pth